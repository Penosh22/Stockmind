{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from typing import Optional\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_experimental.autonomous_agents import BabyAGI\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.agents import AgentExecutor, Tool, ZeroShotAgent\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "from langchain_openai import OpenAI\n",
    "import dotenv\n",
    "import os\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gpt_model = \"gpt-3.5-turbo-0125\"\n",
    "groq_model = \"llama3-8b-8192\"\n",
    "OPENAI_API_KEY = os.getenv(\"groq_api_key\") \n",
    "llm = ChatGroq(model=groq_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nREVIEWS_CSV_PATH = \"E:\\\\Data Science\\\\Capstone\\\\data\"\\n# 1. Load, chunk and index the contents of the blog to create a retriever.\\nloader = CSVLoader(file_path=REVIEWS_CSV_PATH)\\ndocs = loader.load()\\n\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\\nsplits = text_splitter.split_documents(docs)\\nvectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\\nretriever = vectorstore.as_retriever()\\n\\n\\n# 2. Incorporate the retriever into a question-answering chain.\\nsystem_prompt = (\\n    \"You are a professional stock market analyst and investor \"\\n    \"Use the following pieces of retrieved context to answer a question\"\\n    \" If you don\\'t know the answer, say that you \"\\n    \" stocks which have results better than previous results is good \"\\n    \"don\\'t know. Use three sentences maximum and keep the \"\\n    \"answer concise.consider only Indian stockmarket \"\\n    \"Answer only from the content retreived do not answer anything from outside \"\\n    \"\\n\\n\"\\n    \"{context}\"\\n)\\n\\nprompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system_prompt),\\n        (\"human\", \"{input}\"),\\n    ]\\n)\\n\\noutput_parser = StrOutputParser()\\nquestion_answer_chain = create_stuff_documents_chain(llm, prompt)\\nrag_chain = create_retrieval_chain(retriever, question_answer_chain)\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "REVIEWS_CSV_PATH = \"E:\\Data Science\\Capstone\\data\"\n",
    "# 1. Load, chunk and index the contents of the blog to create a retriever.\n",
    "loader = CSVLoader(file_path=REVIEWS_CSV_PATH)\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "# 2. Incorporate the retriever into a question-answering chain.\n",
    "system_prompt = (\n",
    "    \"You are a professional stock market analyst and investor \"\n",
    "    \"Use the following pieces of retrieved context to answer a question\"\n",
    "    \" If you don't know the answer, say that you \"\n",
    "    \" stocks which have results better than previous results is good \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.consider only Indian stockmarket \"\n",
    "    \"Answer only from the content retreived do not answer anything from outside \"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SimpleOutputParser' from 'langchain.output_parsers' (c:\\Users\\PENOSH YADAV\\anaconda3\\envs\\MGI\\Lib\\site-packages\\langchain\\output_parsers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombine_documents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstuff\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StuffDocumentsChain\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_parsers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleOutputParser\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI  \u001b[38;5;66;03m# Assuming you're using OpenAI LLM\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Directory containing CSV files\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'SimpleOutputParser' from 'langchain.output_parsers' (c:\\Users\\PENOSH YADAV\\anaconda3\\envs\\MGI\\Lib\\site-packages\\langchain\\output_parsers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import SimpleOutputParser\n",
    "from langchain.llms import OpenAI  # Assuming you're using OpenAI LLM\n",
    "\n",
    "# Directory containing CSV files\n",
    "directory_path = \"E:\\\\Data Science\\\\Capstone\\\\data\"\n",
    "\n",
    "# Function to load all CSV files from a directory\n",
    "def load_csv_files_from_directory(directory_path):\n",
    "    all_docs = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            loader = CSVLoader(file_path=file_path)\n",
    "            try:\n",
    "                docs = loader.load()\n",
    "                all_docs.extend(docs)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file_path}: {e}\")\n",
    "    return all_docs\n",
    "\n",
    "# Load documents from all CSV files\n",
    "docs = load_csv_files_from_directory(directory_path)\n",
    "\n",
    "# Split the documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Load BERT embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/bert-base-nli-mean-tokens\")\n",
    "\n",
    "# Create a vector store from the chunks\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Define the system prompt and create the QA chain\n",
    "system_prompt = (\n",
    "    \"You are a professional stock market analyst and investor. \"\n",
    "    \"Use the following pieces of retrieved context to answer a question. \"\n",
    "    \"If you don't know the answer, say that you don't know. \"\n",
    "    \"Stocks which have results better than previous results are good. \"\n",
    "    \"Consider only the Indian stock market. \"\n",
    "    \"Answer only from the content retrieved; do not answer anything from outside. \"\n",
    "    \"\\n\\n{context}\"\n",
    ")\n",
    "\n",
    "# Create the LLM (assuming OpenAI here, but replace with your LLM)\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Create the chain for answering\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "output_parser = SimpleOutputParser()\n",
    "stuff_chain = StuffDocumentsChain(prompt=prompt, output_parser=output_parser, llm=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to perform retrieval and generate the answer\n",
    "def answer_question(query):\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "    \n",
    "    # Combine the documents and run the QA chain\n",
    "    answer = stuff_chain.run(input_documents=retrieved_docs, input=query)\n",
    "    return answer\n",
    "\n",
    "# Example usage\n",
    "question = \"What are the best-performing stocks in the Indian market?\"\n",
    "response = answer_question(question)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the text, it is mentioned that the Indian market has shown robust growth with a 46% increase in the past year, and earnings are expected to grow by 16% annually. This suggests that many stocks may have reported better financial results than the previous year.\\n\\nIn particular, the text highlights the Financials and Information Technology sectors, which have gained 1.4% and 3.2%, respectively, over the last 7 days. This implies that some stocks within these sectors may be showing improved financial performance compared to the previous year.\\n\\nHowever, it is not possible to identify specific stocks that have reported better financial results without further information.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"give me stocks which could give massive returns\"})\n",
    "output_parser.parse(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know. The provided context does not mention a specific stock that meets the criteria of having better financial results than the previous year.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"give me stocks which could give massive returns\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "second_question = \"name one stock which meets the criteria?\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_2[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know. The text does not mention stocks from the energy sector.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"stocks in energy sector?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"CAPP\"}\n",
    "    },  \n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I see that NSEI:ONGC is mentioned in the text, which is an energy company.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"any other stock in energy sector?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"CAPP\"}},\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: stocks in energy sector?\n",
      "\n",
      "AI: I don't know. The text does not mention stocks from the energy sector.\n",
      "\n",
      "User: any other stock in energy sector?\n",
      "\n",
      "AI: I see that NSEI:ONGC is mentioned in the text, which is an energy company.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message in store[\"CAPP\"].messages:\n",
    "    if isinstance(message, AIMessage):\n",
    "        prefix = \"AI\"\n",
    "    else:\n",
    "        prefix = \"User\"\n",
    "\n",
    "    print(f\"{prefix}: {message.content}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MGI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
